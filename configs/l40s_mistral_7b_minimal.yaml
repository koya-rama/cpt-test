model:
  name: "mistralai/Mistral-7B-v0.1"
  use_flash_attention: true

data:
  custom_corpus_paths:
    - "./data/custom_corpus"
  text_column: "text"
  max_length: 1024  # Reduced from 2048
  streaming: true
  num_proc: 8

training:
  output_dir: "./checkpoints/l40s_mistral_7b"
  num_epochs: 1
  max_steps: 100
  per_device_train_batch_size: 1  # Reduced from 2
  gradient_accumulation_steps: 16  # Increased from 8
  per_device_eval_batch_size: 1
  learning_rate: 2.0e-05
  weight_decay: 0.01
  max_grad_norm: 1.0
  warmup_steps: 10
  lr_scheduler_type: "cosine"
  optim: "adafactor"  # More memory efficient than AdamW
  bf16: true
  fp16: false
  gradient_checkpointing: true
  logging_steps: 10
  save_steps: 50
  save_total_limit: 1  # Reduced from 2
  evaluation_strategy: "no"
  dataloader_num_workers: 2  # Reduced from 4
  use_wandb: true
  wandb_project: "nemotron-cpt-l40s"
  run_name: "test-mistral-7b-minimal"
